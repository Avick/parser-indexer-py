{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Target Encyclopedia  - NER\n",
    "Thamme Gowda (Thamme.Gowda@jpl.nasa.gov)\n",
    "\n",
    "Named Entity Recognition / Sequence Tagging\n",
    "This notebook contains NER tagging using CRF suite\n",
    "\n",
    "\n",
    "### Notes:\n",
    " + Use python3, Reason: we need unicode strings, which is default in python3\n",
    " + install Python-crfsuite\n",
    " + Start CoreNLP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from codecs import open as copen\n",
    "\n",
    "import os, glob\n",
    "import pickle\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accept_labels = set(['Element', 'Mineral', 'Target', 'Material', 'Locality', 'Site'])\n",
    "accept_labels = set(['Target'])\n",
    "\n",
    "\n",
    "class BratToCRFSuitFeaturizer(object):\n",
    "    def __init__(self, corenlp_url='http://localhost:9000', iob=False):\n",
    "        '''\n",
    "        Create Converter for converting brat annotations to Core NLP NER CRF\n",
    "        classifier training data.\n",
    "        @param corenlp_url: URL to corenlp server.\n",
    "                To start the server checkout: http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "        @param iob: set 'True' for IOB encoding\n",
    "        '''\n",
    "        self.corenlp = StanfordCoreNLP(corenlp_url)\n",
    "        self.iob = iob\n",
    "\n",
    "    def convert(self, text_file, ann_file):\n",
    "        text, tree = self.parse(text_file, ann_file)\n",
    "        props = { 'annotators': 'tokenize,ssplit,lemma,pos', 'outputFormat': 'json'}\n",
    "        if text[0].isspace():\n",
    "            text = '.' + text[1:]\n",
    "            # Reason: some tools trim/strip off the white spaces\n",
    "            # which will mismatch the character offsets\n",
    "        output = self.corenlp.annotate(text, properties=props)\n",
    "        records = []\n",
    "        for sentence in output['sentences']:\n",
    "            sent_features = []\n",
    "            continue_ann, continue_ann_en = None, None\n",
    "            for tok in sentence['tokens']:\n",
    "                begin, tok_end = tok['characterOffsetBegin'], tok['characterOffsetEnd']\n",
    "                label = 'O'\n",
    "                if begin in tree:\n",
    "                    node = tree[begin]\n",
    "                    if len(node) > 1:\n",
    "                        print(\"WARN: multiple starts at \", begin, node)\n",
    "                        if tok_end in node:\n",
    "                            node = {tok_end: node[tok_end]} # picking one\n",
    "                            print(\"Chose:\", node)\n",
    "\n",
    "                    ann_end, labels = list(node.items())[0]\n",
    "                    if not len(labels) == 1:\n",
    "                        print(\"WARN: Duplicate labels for token: %s, label:%s.\\\n",
    "                              Using the first one!\" % (tok['word'], str(labels)))\n",
    "                    if accept_labels is not None and labels[0] in accept_labels:\n",
    "                        label = labels[0]\n",
    "\n",
    "                    if tok_end == ann_end: # annotation ends where token ends\n",
    "                        continue_ann = None\n",
    "                    elif tok_end < ann_end and label != 'O':\n",
    "                        #print(\"Continue for the next %d chars\" % (ann_end - tok_end))\n",
    "                        continue_ann = label\n",
    "                        continue_ann_end = ann_end \n",
    "                    if label != 'O' and self.iob:\n",
    "                        label = \"B-\" + label\n",
    "                elif continue_ann is not None and tok_end <= continue_ann_end:\n",
    "                    #print(\"Continuing the annotation %s, %d:%d %d]\" % \n",
    "                    #(continue_ann, begin, tok_end, continue_ann_end))\n",
    "                    label = continue_ann            # previous label is this label\n",
    "                    if continue_ann_end == tok_end: # continuation ends here\n",
    "                        #print(\"End\")\n",
    "                        continue_ann = None\n",
    "                    if self.iob:\n",
    "                        label = \"I-\" + label\n",
    "                sent_features.append([tok['word'], tok['lemma'], tok['pos'], label])\n",
    "            yield sent_features\n",
    "\n",
    "\n",
    "    def parse(self, txt_file, ann_file):\n",
    "        with copen(ann_file, 'r', encoding='utf-8') as ann_file:\n",
    "            with copen(txt_file, 'r', encoding='utf-8') as text_file:\n",
    "                texts = text_file.read()\n",
    "            anns = map(lambda x: x.strip().split('\\t'), ann_file)\n",
    "            anns = filter(lambda x: len(x) > 2, anns)\n",
    "            # FIXME: ignoring the annotatiosn which are complex\n",
    "\n",
    "            anns = filter(lambda x: ';' not in x[1], anns)\n",
    "            # FIXME: some annotations' spread have been split into many, separated by ; ignoring them\n",
    "\n",
    "            def __parse_ann(ann):\n",
    "                spec = ann[1].split()\n",
    "                name = spec[0]\n",
    "                markers = list(map(lambda x: int(x), spec[1:]))\n",
    "                #t = ' '.join([texts[begin:end] for begin,end in zip(markers[::2], markers[1::2])])\n",
    "                t = texts[markers[0]:markers[1]]\n",
    "                if not t == ann[2]:\n",
    "                    print(\"Error: Annotation mis-match, file=%s, ann=%s\" % (txt_file, str(ann)))\n",
    "                    return None\n",
    "                return (name, markers, t)\n",
    "            anns = map(__parse_ann, anns) # format\n",
    "            anns = filter(lambda x: x, anns) # skip None\n",
    "\n",
    "            # building a tree index for easy accessing\n",
    "            tree = {}\n",
    "            for entity_type, pos, name in anns:\n",
    "                begin, end = pos[0], pos[1]\n",
    "                if begin not in tree:\n",
    "                    tree[begin] = {}\n",
    "                node = tree[begin]\n",
    "                if end not in node:\n",
    "                    node[end] = []\n",
    "                node[end].append(entity_type)\n",
    "\n",
    "            # Re-read file in without decoding it\n",
    "            text_file = copen(txt_file, 'r', encoding='utf-8')\n",
    "            texts = text_file.read()\n",
    "            text_file.close()\n",
    "            return texts, tree\n",
    "\n",
    "def scan_dir(dir_name):\n",
    "    items = glob.glob(dir_name + \"/*.ann\")\n",
    "    items = map(lambda f: (f, f.replace(\".ann\", \".txt\")), items)\n",
    "    return items\n",
    "\n",
    "def preprocess_all(list_file, out_file):\n",
    "    featzr = BratToCRFSuitFeaturizer(iob=True)\n",
    "    tokenized = []\n",
    "    with open(list_file) as f:\n",
    "        examples = map(lambda l:l.strip().split(','), f.readlines())\n",
    "    for txt_file, ann_file in examples:\n",
    "        sents = featzr.convert(txt_file, ann_file)\n",
    "        tokenized.append(list(sents))\n",
    "\n",
    "    pickle.dump(tokenized, open(out_file, 'wb'))\n",
    "    print(\"Dumped %d docs to %s\" % (len(tokenized), out_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and store the corpus\n",
    "\n",
    "In this step, we pass the text through CoreNLP pipeline, tokenize and POS tag them. \n",
    "In addition, we lookup the annotations file and match the target annotations with the token. \n",
    "\n",
    "Since this step is expensive, we store the results in pickle file, so that we can later load and resume our analysis for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_dir = \"/Users/thammegr/work/mte/data/newcorpus/workspace\"\n",
    "train_list = p_dir + \"/train_62r15_685k14_384k15.list\"\n",
    "dev_list= p_dir + \"/development.list\"\n",
    "test_list = p_dir + \"/test.list\"\n",
    "\n",
    "train_corpus_file = 'mte-corpus-train.pickle'\n",
    "preprocess_all(train_list, train_corpus_file)\n",
    "\n",
    "# Test and Development set\n",
    "dev_corpus_file = 'mte-corpus-dev.pickle'\n",
    "preprocess_all(dev_list, dev_corpus_file)\n",
    "test_corpus_file = 'mte-corpus-test.pickle'\n",
    "preprocess_all(test_list, test_corpus_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus\n",
    "Here we load the corpus from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hollow', 'hollow', 'JJ', 'O'],\n",
       " ['spherical', 'spherical', 'JJ', 'O'],\n",
       " ['feature', 'feature', 'NN', 'O'],\n",
       " ['observed', 'observe', 'VBN', 'O'],\n",
       " ['on', 'on', 'IN', 'O'],\n",
       " ['sol', 'sol', 'NN', 'O'],\n",
       " ['122', '122', 'CD', 'O'],\n",
       " ['in', 'in', 'IN', 'O'],\n",
       " ['the', 'the', 'DT', 'O'],\n",
       " ['Yellowknife', 'Yellowknife', 'NNP', 'O'],\n",
       " ['Bay', 'Bay', 'NNP', 'O'],\n",
       " ['area', 'area', 'NN', 'O'],\n",
       " ['.', '.', '.', 'O']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = 'mte-corpus-train.pickle'\n",
    "corpus = pickle.load(open(corpus_file, 'rb'))\n",
    "corpus[0][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start playing with the features of CRF Suite to build a sequence tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature.minfreq', 'feature.possible_states', 'feature.possible_transitions', 'c1', 'c2', 'max_iterations', 'num_memories', 'epsilon', 'period', 'delta', 'linesearch', 'max_linesearch']\n",
      "Training Done\n",
      "-rw-r--r--  1 thammegr  703763885   214K Feb 16 08:25 jpl-mars-target-ner-model.crfsuite\n",
      "CPU times: user 2min 13s, sys: 605 ms, total: 2min 13s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def word2features(sent, idx):\n",
    "    word = sent[idx]\n",
    "    feats = [\n",
    "        'word.bias',\n",
    "        'word.lemma=' + word[1],\n",
    "        'word.pos=' + word[2],\n",
    "        'word.lower=' + word[0].lower(),\n",
    "        'word[-3:]=' + word[0][-3:],\n",
    "        'word[-2:]=' + word[0][-2:],\n",
    "        'word.isupper=%s' % word[0].isupper(),\n",
    "        'word.istitle=%s' % word[0].istitle(),\n",
    "        'word.isdigit=%s' % word[0].isdigit(),\n",
    "    ]\n",
    "    if idx > 0:\n",
    "        word = sent[idx-1]\n",
    "        feats.extend([\n",
    "            '-1:word.bias',\n",
    "            '-1:word.lemma=' + word[1],\n",
    "            '-1:word.pos=' + word[2],\n",
    "            '-1:word.lower=' + word[0].lower(),\n",
    "            '-1:word[-3:]=' + word[0][-3:],\n",
    "            '-1:word[-2:]=' + word[0][-2:],\n",
    "            '-1:word.isupper=%s' % word[0].isupper(),\n",
    "            '-1:word.istitle=%s' % word[0].istitle(),\n",
    "            '-1:word.isdigit=%s' % word[0].isdigit(),\n",
    "        ])\n",
    "    else:\n",
    "        feats.append('BOS')\n",
    "    if idx < len(sent) - 1:\n",
    "        word = sent[idx + 1]\n",
    "        feats.extend([\n",
    "            '+1:word.bias',\n",
    "            '+1:word.lemma=' + word[1],\n",
    "            '+1:word.pos=' + word[2],\n",
    "            '+1:word.lower=' + word[0].lower(),\n",
    "            '+1:word[-3:]=' + word[0][-3:],\n",
    "            '+1:word[-2:]=' + word[0][-2:],\n",
    "            '+1:word.isupper=%s' % word[0].isupper(),\n",
    "            '+1:word.istitle=%s' % word[0].istitle(),\n",
    "            '+1:word.isdigit=%s' % word[0].isdigit(),\n",
    "        ])\n",
    "    else:\n",
    "        feats.append('EOS')\n",
    "    return feats\n",
    "\n",
    "def seq2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def seq2labels(sent):\n",
    "    return [tok[3] for tok in sent]\n",
    "\n",
    "def merge_sequences(doc):\n",
    "    '''\n",
    "    document contains multiple sentences. here all sentences in document are merged to form one large sequence.\n",
    "    '''\n",
    "    res = []\n",
    "    for seq in doc:\n",
    "        res.extend(seq)\n",
    "        res.append(['|', '|', '|', 'O']) # sentence end marker\n",
    "    return res\n",
    "    \n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for doc in corpus:\n",
    "    seq = merge_sequences(doc)\n",
    "    x_seq = seq2features(seq)\n",
    "    y_seq = seq2labels(seq)\n",
    "    trainer.append(x_seq, y_seq)\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 0.5,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "print(trainer.params())\n",
    "model_file = 'jpl-mars-target-ner-model.crfsuite'\n",
    "trainer.train(model_file)\n",
    "print(\"Training Done\")\n",
    "\n",
    "!ls -alh 'jpl-mars-target-ner-model.crfsuite'\n",
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using the model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth, Predicted, [******]\n",
      "337 B-Target B-Target ['Confidence', 'Confidence', 'NNP', 'B-Target'] \n",
      "338 I-Target I-Target ['Hills', 'Hills', 'NNP', 'I-Target'] \n",
      "538 B-Target B-Target ['Confidence', 'confidence', 'NN', 'B-Target'] \n",
      "539 I-Target I-Target ['Hill', 'Hill', 'NNP', 'I-Target'] \n",
      "712 B-Target O ['Shoemaker', 'Shoemaker', 'NNP', 'B-Target'] <<<<<ERROR\n",
      "714 B-Target B-Target ['Alexander', 'Alexander', 'NNP', 'B-Target'] \n",
      "715 I-Target I-Target ['Hills', 'Hills', 'NNP', 'I-Target'] \n",
      "718 B-Target B-Target ['Chinle', 'Chinle', 'NNP', 'B-Target'] \n",
      "735 B-Target B-Target ['Pink', 'Pink', 'NNP', 'B-Target'] \n",
      "736 I-Target I-Target ['Cliffs', 'Cliffs', 'NNPS', 'I-Target'] \n",
      "738 B-Target B-Target ['Alexander', 'Alexander', 'NNP', 'B-Target'] \n",
      "739 I-Target I-Target ['Hills', 'Hills', 'NNP', 'I-Target'] \n",
      "780 B-Target B-Target ['Pink', 'Pink', 'NNP', 'B-Target'] \n",
      "781 I-Target I-Target ['Cliffs', 'Cliffs', 'NNPS', 'I-Target'] \n",
      "783 B-Target B-Target ['Book', 'book', 'VB', 'B-Target'] \n",
      "784 I-Target I-Target ['Cliffs', 'Cliffs', 'NNPS', 'I-Target'] \n",
      "786 B-Target B-Target ['Alexander', 'Alexander', 'NNP', 'B-Target'] \n",
      "787 I-Target I-Target ['Hills', 'Hills', 'NNP', 'I-Target'] \n",
      "790 B-Target B-Target ['Carnivore', 'Carnivore', 'NNP', 'B-Target'] \n",
      "791 I-Target I-Target ['Canyon', 'Canyon', 'NNP', 'I-Target'] \n",
      "807 B-Target B-Target ['Book', 'book', 'NN', 'B-Target'] \n",
      "808 I-Target I-Target ['Cliffs', 'Cliffs', 'NNPS', 'I-Target'] \n",
      "810 B-Target B-Target ['Alexander', 'Alexander', 'NNP', 'B-Target'] \n",
      "811 I-Target I-Target ['Hills', 'Hills', 'NNP', 'I-Target'] \n",
      "814 B-Target B-Target ['Carnivore', 'Carnivore', 'NNP', 'B-Target'] \n",
      "815 I-Target I-Target ['Canyon', 'Canyon', 'NNP', 'I-Target'] \n",
      "872 B-Target B-Target ['Chinle', 'Chinle', 'NNP', 'B-Target'] \n",
      "909 B-Target B-Target ['Whale', 'Whale', 'NNP', 'B-Target'] \n",
      "910 I-Target I-Target ['Rock', 'Rock', 'NNP', 'I-Target'] \n",
      "937 B-Target B-Target ['Whale', 'Whale', 'NNP', 'B-Target'] \n",
      "938 I-Target I-Target ['Rock', 'Rock', 'NNP', 'I-Target'] \n",
      "1078 B-Target B-Target ['Chinle', 'Chinle', 'NNP', 'B-Target'] \n",
      "1095 B-Target B-Target ['Whale', 'Whale', 'NNP', 'B-Target'] \n",
      "1096 I-Target I-Target ['Rock', 'Rock', 'NNP', 'I-Target'] \n",
      "1269 B-Target B-Target ['Chinle', 'Chinle', 'NNP', 'B-Target'] \n",
      "1280 B-Target B-Target ['Whale', 'Whale', 'NNP', 'B-Target'] \n",
      "1281 I-Target I-Target ['Rock', 'Rock', 'NNP', 'I-Target'] \n",
      "37\n"
     ]
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(model_file)\n",
    "doc = corpus[20]\n",
    "seq = merge_sequences(doc)\n",
    "\n",
    "y = seq2labels(seq)\n",
    "y_ = tagger.tag(seq2features(seq))\n",
    "\n",
    "c = 0\n",
    "print(\"Truth, Predicted, [******]\")\n",
    "for idx, a,p, tok in zip(range(len(seq)), y, y_, seq):\n",
    "    if a != 'O' or p != 'O':\n",
    "        print(idx, a, p, tok, \"<<<<<ERROR\" if a != p else \"\")\n",
    "        c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   B-Target       0.95      0.24      0.39       147\n",
      "   I-Target       0.88      0.50      0.64        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00     34970\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   B-Target       0.95      0.30      0.46       194\n",
      "   I-Target       1.00      0.35      0.52        20\n",
      "\n",
      "avg / total       1.00      1.00      1.00     60630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    #tagset.append('O')\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "\n",
    "def evaluate(tagger, corpus_file):    \n",
    "    corpus = pickle.load(open(corpus_file, 'rb'))\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for doc in corpus:\n",
    "        seq = merge_sequences(doc)\n",
    "        y_true.append(seq2labels(seq))\n",
    "        y_pred.append(tagger.tag(seq2features(seq)))\n",
    "    return bio_classification_report(y_true, y_pred)\n",
    "\n",
    "\n",
    "dev_corpus_file = 'mte-corpus-dev.pickle'\n",
    "test_corpus_file = 'mte-corpus-test.pickle'\n",
    "print(evaluate(tagger, dev_corpus_file))\n",
    "print(evaluate(tagger, test_corpus_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning: State Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-Target -> I-Target 2.317607\n",
      "I-Target -> I-Target 1.922520\n",
      "O      -> O       1.387535\n",
      "B-Target -> B-Target 1.191201\n",
      "I-Target -> B-Target 1.121970\n",
      "I-Target -> O       -0.366761\n",
      "O      -> B-Target -0.370077\n",
      "B-Target -> O       -2.414795\n",
      "O      -> I-Target -6.240270\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-Target -> I-Target 2.317607\n",
      "I-Target -> I-Target 1.922520\n",
      "O      -> O       1.387535\n",
      "B-Target -> B-Target 1.191201\n",
      "I-Target -> B-Target 1.121970\n",
      "I-Target -> O       -0.366761\n",
      "O      -> B-Target -0.370077\n",
      "B-Target -> O       -2.414795\n",
      "O      -> I-Target -6.240270\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning: State Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "6.334475 B-Target word.lower=yarrada\n",
      "6.264323 B-Target word.lower=dillinger\n",
      "5.925225 B-Target word.lower=bathurst\n",
      "5.814473 B-Target word.lower=kilian\n",
      "4.872865 B-Target word.lower=mammoth\n",
      "4.838683 B-Target word.lower=horseback\n",
      "4.582175 B-Target word.lower=epworth\n",
      "4.582175 B-Target word.lemma=Epworth\n",
      "4.470880 B-Target word.lower=kenwood_river\n",
      "4.439386 B-Target word.lower=castle_mountain\n",
      "4.133225 B-Target word.lower=chakonipau\n",
      "4.070196 B-Target word.lower=bathurst_inlet\n",
      "4.034428 B-Target word.lower=mondooma\n",
      "4.034428 B-Target word.lemma=mondooma\n",
      "4.032540 B-Target word.lower=hottah\n",
      "3.900069 B-Target word.lower=windjana\n",
      "3.869020 B-Target word.lower=maturango\n",
      "3.838600 B-Target word.lower=watchtower\n",
      "3.807808 B-Target word.lemma=bonanza_king\n",
      "3.807808 B-Target word.lower=bonanza_king\n",
      "\n",
      "Top negative:\n",
      "-1.859349 O      word.lower=cb\n",
      "-1.892228 O      +1:word[-2:]=ut\n",
      "-1.894719 O      word.lower=neil\n",
      "-1.894719 O      word.lemma=Neil\n",
      "-1.926752 O      word[-2:]=CB\n",
      "-1.982927 O      word[-2:]=-5\n",
      "-2.075573 O      -1:word.lemma=conglomerate\n",
      "-2.227942 O      word.lower=mara\n",
      "-2.283856 O      word.lower=portage\n",
      "-2.339135 O      word[-2:]=ak\n",
      "-2.404078 O      word.lower=jake\n",
      "-2.435240 O      word.lower=sheepbed\n",
      "-2.542449 O      word[-3:]=yon\n",
      "-2.873553 O      word.lower=comanche\n",
      "-2.982831 O      -1:word.lemma=H\n",
      "-3.022433 O      word.lower=cumberland\n",
      "-3.193014 O      word[-3:]=out\n",
      "-3.212225 O      +1:word.lower=subclass\n",
      "-3.491257 O      word.lower=watchtower\n",
      "-3.895909 O      word.lower=windjana\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
