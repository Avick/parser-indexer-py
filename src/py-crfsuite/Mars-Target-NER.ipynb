{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding::utf-8\n",
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from codecs import open as copen\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "import sys\n",
    "print(\"Encoding::\" + sys.getdefaultencoding())\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accept_labels = set(['Element', 'Mineral', 'Target', 'Material', 'Locality', 'Site'])\n",
    "accept_labels = set(['Target'])\n",
    "\n",
    "\n",
    "class BratToCRFSuitFeaturizer(object):\n",
    "    def __init__(self, corenlp_url='http://localhost:9000', iob=False):\n",
    "        '''\n",
    "        Create Converter for converting brat annotations to Core NLP NER CRF\n",
    "        classifier training data.\n",
    "        @param corenlp_url: URL to corenlp server.\n",
    "                To start the server checkout: http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "        @param iob: set 'True' for IOB encoding\n",
    "        '''\n",
    "        self.corenlp = StanfordCoreNLP(corenlp_url)\n",
    "        self.iob = iob\n",
    "\n",
    "    def convert(self, text_file, ann_file):\n",
    "        text, tree = self.parse(text_file, ann_file)\n",
    "        props = { 'annotators': 'tokenize,ssplit,lemma,pos', 'outputFormat': 'json'}\n",
    "        if text[0].isspace():\n",
    "            text = '.' + text[1:]\n",
    "            # Reason: some tools trim/strip off the white spaces\n",
    "            # which will mismatch the character offsets\n",
    "        output = self.corenlp.annotate(text, properties=props)\n",
    "        records = []\n",
    "        for sentence in output['sentences']:\n",
    "            sent_features = []\n",
    "            continue_ann, continue_ann_en = None, None\n",
    "            for tok in sentence['tokens']:\n",
    "                begin, tok_end = tok['characterOffsetBegin'], tok['characterOffsetEnd']\n",
    "                label = 'O'\n",
    "                if begin in tree:\n",
    "                    node = tree[begin]\n",
    "                    if len(node) > 1:\n",
    "                        print(\"WARN: multiple starts at \", begin, node)\n",
    "                        if tok_end in node:\n",
    "                            node = {tok_end: node[tok_end]} # picking one\n",
    "                            print(\"Chose:\", node)\n",
    "\n",
    "                    ann_end, labels = list(node.items())[0]\n",
    "                    if not len(labels) == 1:\n",
    "                        print(\"WARN: Duplicate labels for token: %s, label:%s. Using the first one!\" % (tok['word'], str(labels)))\n",
    "                    if accept_labels is not None and labels[0] in accept_labels:\n",
    "                        label = labels[0]\n",
    "\n",
    "                    if tok_end == ann_end: # annotation ends where token ends\n",
    "                        continue_ann = None\n",
    "                    elif tok_end < ann_end and label != 'O':\n",
    "                        #print(\"Continue for the next %d chars\" % (ann_end - tok_end))\n",
    "                        continue_ann = label\n",
    "                        continue_ann_end = ann_end \n",
    "                    if label != 'O' and self.iob:\n",
    "                        label = \"B-\" + label\n",
    "                elif continue_ann is not None and tok_end <= continue_ann_end:\n",
    "                    #print(\"Continuing the annotation %s, %d:%d %d]\" % (continue_ann, begin, tok_end, continue_ann_end))\n",
    "                    label = continue_ann            # previous label is this label\n",
    "                    if continue_ann_end == tok_end: # continuation ends here\n",
    "                        #print(\"End\")\n",
    "                        continue_ann = None\n",
    "                    if self.iob:\n",
    "                        label = \"I-\" + label\n",
    "                sent_features.append((tok['word'], label, tok['lemma'], tok['pos']))\n",
    "            yield sent_features\n",
    "\n",
    "\n",
    "    def parse(self, txt_file, ann_file):\n",
    "        with open(txt_file, 'r', 'utf-8') as text_file, open(ann_file, 'r', 'utf-8') as ann_file:\n",
    "            texts = text_file.read()\n",
    "            text_file.close()\n",
    "            #texts = text_file.read()\n",
    "            anns = map(lambda x: x.strip().split('\\t'), ann_file)\n",
    "            anns = filter(lambda x: len(x) > 2, anns)\n",
    "            # FIXME: ignoring the annotatiosn which are complex\n",
    "\n",
    "            anns = filter(lambda x: ';' not in x[1], anns)\n",
    "            # FIXME: some annotations' spread have been split into many, separated by ; ignoring them\n",
    "\n",
    "            def __parse_ann(ann):\n",
    "                spec = ann[1].split()\n",
    "                name = spec[0]\n",
    "                markers = list(map(lambda x: int(x), spec[1:]))\n",
    "                #t = ' '.join([texts[begin:end] for begin,end in zip(markers[::2], markers[1::2])])\n",
    "                t = texts[markers[0]:markers[1]]\n",
    "                if not t == ann[2]:\n",
    "                    print(\"Error: Annotation mis-match, file=%s, ann=%s\" % (txt_file, str(ann)))\n",
    "                    return None\n",
    "                return (name, markers, t)\n",
    "            anns = map(__parse_ann, anns) # format\n",
    "            anns = filter(lambda x: x, anns) # skip None\n",
    "\n",
    "            # building a tree index for easy accessing\n",
    "            tree = {}\n",
    "            for entity_type, pos, name in anns:\n",
    "                begin, end = pos[0], pos[1]\n",
    "                if begin not in tree:\n",
    "                    tree[begin] = {}\n",
    "                node = tree[begin]\n",
    "                if end not in node:\n",
    "                    node[end] = []\n",
    "                node[end].append(entity_type)\n",
    "\n",
    "            # Re-read file in without decoding it\n",
    "            text_file = open(txt_file)\n",
    "            texts = text_file.read()\n",
    "            text_file.close()\n",
    "            return texts, tree\n",
    "\n",
    "    def convert_all(self, input_paths, output):\n",
    "        with open(input_paths) as paths, open(output, 'w') as out:\n",
    "            for p in map(lambda x: x.strip(), paths):\n",
    "                d = p.split(',')\n",
    "                print(d)\n",
    "                for line in self.convert(d[0], d[1]):\n",
    "                    out.write(line)\n",
    "                    out.write(\"\\n\")\n",
    "                out.write(\"\\n\") # end of document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'O', 'the', 'DT')\n",
      "('red', 'O', 'red', 'JJ')\n",
      "('dots', 'O', 'dot', 'NNS')\n",
      "('represent', 'O', 'represent', 'VBP')\n",
      "('points', 'O', 'point', 'NNS')\n",
      "('where', 'O', 'where', 'WRB')\n",
      "('contact', 'O', 'contact', 'NN')\n",
      "('science', 'O', 'science', 'NN')\n",
      "('was', 'O', 'be', 'VBD')\n",
      "('performed', 'O', 'perform', 'VBN')\n",
      "('during', 'O', 'during', 'IN')\n",
      "('loop', 'O', 'loop', 'NN')\n",
      "('2', 'O', '2', 'CD')\n",
      "('and', 'O', 'and', 'CC')\n",
      "('the', 'O', 'the', 'DT')\n",
      "('blue', 'O', 'blue', 'JJ')\n",
      "('dot', 'O', 'dot', 'NN')\n",
      "('represents', 'O', 'represent', 'VBZ')\n",
      "('the', 'O', 'the', 'DT')\n",
      "('location', 'O', 'location', 'NN')\n",
      "('of', 'O', 'of', 'IN')\n",
      "('a', 'O', 'a', 'DT')\n",
      "('drill', 'O', 'drill', 'NN')\n",
      "('campaign', 'O', 'campaign', 'NN')\n",
      "('.', 'O', '.', '.')\n"
     ]
    }
   ],
   "source": [
    "def word2features(sent, idx):\n",
    "    pass\n",
    "\n",
    "def sentence2features(sent):\n",
    "    pass\n",
    "\n",
    "p_dir = \"/Users/thammegr/work/mte/data/lpsc15-C-raymond-sol1159\"\n",
    "txt_file = p_dir + \"/2855.txt\"\n",
    "ann_file = p_dir + \"/2855.ann\"\n",
    "txts, tree = read_brat_ann(txt_file, ann_file)\n",
    "\n",
    "featzr = BratToCRFSuitFeaturizer(iob=True)\n",
    "train_sents = list(featzr.convert(txt_file, ann_file))\n",
    "\n",
    "print(\"\\n\".join(map(str, train_sents[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
