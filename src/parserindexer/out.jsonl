{"content": "Exploiting Shallow Linguistic Information for\nRelation Extraction from Biomedical Literature\n\nClaudio Giuliano and Alberto Lavelli and Lorenza Romano\nITC-irst\n\nVia Sommarive, 18\n38050, Povo (TN)\n\nItaly\n{giuliano,lavelli,romano}@itc.it\n\nAbstract\n\nWe propose an approach for extracting re-\nlations between entities from biomedical\nliterature based solely on shallow linguis-\ntic information. We use a combination of\nkernel functions to integrate two different\ninformation sources: (i) the whole sen-\ntence where the relation appears, and (ii)\nthe local contexts around the interacting\nentities. We performed experiments on ex-\ntracting gene and protein interactions from\ntwo different data sets. The results show\nthat our approach outperforms most of the\nprevious methods based on syntactic and\nsemantic information.\n\n1 Introduction\n\nInformation Extraction (IE) is the process of find-\ning relevant entities and their relationships within\ntextual documents. Applications of IE range from\nSemantic Web to Bioinformatics. For example,\nthere is an increasing interest in automatically\nextracting relevant information from biomedi-\ncal literature. Recent evaluation campaigns on\nbio-entity recognition, such as BioCreAtIvE and\nJNLPBA 2004 shared task, have shown that sev-\neral systems are able to achieve good performance\n(even if it is a bit worse than that reported on news\narticles). However, relation identification is more\nuseful from an applicative perspective but it is still\na considerable challenge for automatic tools.\nIn this work, we propose a supervised machine\n\nlearning approach to relation extraction which is\napplicable even when (deep) linguistic process-\ning is not available or reliable. In particular, we\nexplore a kernel-based approach based solely on\nshallow linguistic processing, such as tokeniza-\n\ntion, sentence splitting, Part-of-Speech (PoS) tag-\nging and lemmatization.\nKernel methods (Shawe-Taylor and Cristianini,\n\n2004) show their full potential when an explicit\ncomputation of the feature map becomes compu-\ntationally infeasible, due to the high or even infi-\nnite dimension of the feature space. For this rea-\nson, kernels have been recently used to develop\ninnovative approaches to relation extraction based\non syntactic information, in which the examples\npreserve their original representations (i.e. parse\ntrees) and are compared by the kernel function\n(Zelenko et al., 2003; Culotta and Sorensen, 2004;\nZhao and Grishman, 2005).\nDespite the positive results obtained exploiting\n\nsyntactic information, we claim that there is still\nroom for improvement relying exclusively on shal-\nlow linguistic information for two main reasons.\nFirst of all, previous comparative evaluations put\nmore stress on the deep linguistic approaches and\ndid not put as much effort on developing effec-\ntive methods based on shallow linguistic informa-\ntion. A second reason concerns the fact that syn-\ntactic parsing is not always robust enough to deal\nwith real-world sentences. This may prevent ap-\nproaches based on syntactic features from produc-\ning any result. Another related issue concerns the\nfact that parsers are available only for few lan-\nguages and may not produce reliable results when\nused on domain specific texts (as is the case of\nthe biomedical literature). For example, most of\nthe participants at the Learning Language in Logic\n(LLL) challenge on Genic Interaction Extraction\n(see Section 4.2) were unable to successfully ex-\nploit linguistic information provided by parsers. It\nis still an open issue whether the use of domain-\nspecific treebanks (such as the Genia treebank1)\n\n1http://www-tsujii.is.s.u-tokyo.ac.jp/\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\n\n\ncan be successfully exploited to overcome this\nproblem. Therefore it is essential to better investi-\ngate the potential of approaches based exclusively\non simple linguistic features.\nIn our approach we use a combination of ker-\n\nnel functions to represent two distinct informa-\ntion sources: the global context where entities ap-\npear and their local contexts. The whole sentence\nwhere the entities appear (global context) is used\nto discover the presence of a relation between two\nentities, similarly to what was done by Bunescu\nand Mooney (2005b). Windows of limited size\naround the entities (local contexts) provide use-\nful clues to identify the roles of the entities within\na relation. The approach has some resemblance\nwith what was proposed by Roth and Yih (2002).\nThe main difference is that we perform the extrac-\ntion task in a single step via a combined kernel,\nwhile they used two separate classifiers to identify\nentities and relations and their output is later com-\nbined with a probabilistic global inference.\nWe evaluated our relation extraction algorithm\n\non two biomedical data sets (i.e. the AImed cor-\npus and the LLL challenge data set; see Section\n4). The motivations for using these benchmarks\nderive from the increasing applicative interest in\ntools able to extract relations between relevant en-\ntities in biomedical texts and, consequently, from\nthe growing availability of annotated data sets.\nThe experiments show clearly that our approach\nconsistently improves previous results. Surpris-\ningly, it outperforms most of the systems based on\nsyntactic or semantic information, even when this\ninformation is manually annotated (i.e. the LLL\nchallenge).\n\n2 Problem Formalization\n\nThe problem considered here is that of iden-\ntifying interactions between genes and proteins\nfrom biomedical literature. More specifically, we\nperformed experiments on two slightly different\nbenchmark data sets (see Section 4 for a detailed\ndescription). In the former (AImed) gene/protein\ninteractions are annotated without distinguishing\nthe type and roles of the two interacting entities.\nThe latter (LLL challenge) is more realistic (and\ncomplex) because it also aims at identifying the\nroles played by the interacting entities (agent and\ntarget). For example, in Figure 1 three entities\nare mentioned and two of the six ordered pairs of\n\nGENIA/topics/Corpus/GTB.html\n\nentities actually interact: (sigma(K), cwlH) and\n(gerE, cwlH).\n\nFigure 1: A sentence with two relations, R12 and\nR32, between three entities, E1, E2 and E3.\n\nIn our approach we cast relation extraction as a\nclassification problem, in which examples are gen-\nerated from sentences as follows.\nFirst of all, we describe the complex case,\n\nnamely the protein/gene interactions (LLL chal-\nlenge). For this data set entity recognition is per-\nformed using a dictionary of protein and gene\nnames in which the type of the entities is unknown.\nWe generate examples for all the sentences con-\ntaining at least two entities. Thus the number of\nexamples generated for each sentence is given by\nthe combinations of distinct entities (N ) selected\ntwo at a time, i.e. NC2. For example, as the sen-\ntence shown in Figure 1 contains three entities, the\ntotal number of examples generated is 3C2 = 3. In\neach example we assign the attribute CANDIDATE\nto each of the candidate interacting entities, while\nthe other entities in the example are assigned the\nattribute OTHER, meaning that they do not partici-\npate in the relation. If a relation holds between the\ntwo candidate interacting entities the example is\nlabeled 1 or 2 (according to the roles of the inter-\nacting entities, agent and target, i.e. to the direc-\ntion of the relation); 0 otherwise. Figure 2 shows\nthe examples generated from the sentence in Fig-\nure 1.\n\nFigure 2: The three protein-gene examples gener-\nated from the sentence in Figure 1.\n\nNote that in generating the examples from the\nsentence in Figure 1 we did not create three neg-\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\n\n\native examples (there are six potential ordered re-\nlations between three entities), thereby implicitly\nunder-sampling the data set. This allows us to\nmake the classification task simpler without loos-\ning information. As a matter of fact, generating\nexamples for each ordered pair of entities would\nproduce two subsets of the same size containing\nsimilar examples (differing only for the attributes\nCANDIDATE and OTHER), but with different clas-\nsification labels. Furthermore, under-sampling al-\nlows us to halve the data set size and reduce the\ndata skewness.\nFor the protein-protein interaction task (AImed)\n\nwe use the correct entities provided by the manual\nannotation. As said at the beginning of this sec-\ntion, this task is simpler than the LLL challenge\nbecause there is no distinction between types (all\nentities are proteins) and roles (the relation is sym-\nmetric). As a consequence, the examples are gen-\nerated as described above with the following dif-\nference: an example is labeled 1 if a relation holds\nbetween the two candidate interacting entities; 0\notherwise.\n\n3 Kernel Methods for Relation\nExtraction\n\nThe basic idea behind kernel methods is to embed\nthe input data into a suitable feature space F via\na mapping function \ufffd : X ! F , and then use\na linear algorithm for discovering nonlinear pat-\nterns. Instead of using the explicit mapping \ufffd, we\ncan use a kernel function K : X \u21e5 X ! R, that\ncorresponds to the inner product in a feature space\nwhich is, in general, different from the input space.\nKernel methods allow us to design a modular\n\nsystem, in which the kernel function acts as an\ninterface between the data and the learning algo-\nrithm. Thus the kernel function is the only domain\nspecific module of the system, while the learning\nalgorithm is a general purpose component. Po-\ntentially any kernel function can work with any\nkernel-based algorithm. In our approach we use\nSupport Vector Machines (Vapnik, 1998).\nIn order to implement the approach based on\n\nshallow linguistic information we employed a\nlinear combination of kernels. Different works\n(Gliozzo et al., 2005; Zhao and Grishman, 2005;\nCulotta and Sorensen, 2004) empirically demon-\nstrate the effectiveness of combining kernels in\nthis way, showing that the combined kernel always\nimproves the performance of the individual ones.\n\nIn addition, this formulation allows us to evalu-\nate the individual contribution of each informa-\ntion source. We designed two families of kernels:\nGlobal Context kernels and Local Context kernels,\nin which each single kernel is explicitly calculated\nas follows\n\nK(x1, x2) =\nh\ufffd(x1),\ufffd(x2)i\nk\ufffd(x1)kk\ufffd(x2)k\n\n, (1)\n\nwhere \ufffd(\u00b7) is the embedding vector and k \u00b7 k is the\n2-norm. The kernel is normalized (divided) by the\nproduct of the norms of embedding vectors. The\nnormalization factor plays an important role in al-\nlowing us to integrate information from heteroge-\nneous feature spaces. Even though the resulting\nfeature space has high dimensionality, an efficient\ncomputation of Equation 1 can be carried out ex-\nplicitly since the input representations defined be-\nlow are extremely sparse.\n\n3.1 Global Context Kernel\nIn (Bunescu and Mooney, 2005b), the authors ob-\nserved that a relation between two entities is gen-\nerally expressed using only words that appear si-\nmultaneously in one of the following three pat-\nterns:\n\nFore-Between: tokens before and between the\ntwo candidate interacting entities. For in-\nstance: binding of [P1] to [P2], interaction in-\nvolving [P1] and [P2], association of [P1] by\n[P2].\n\nBetween: only tokens between the two candidate\ninteracting entities. For instance: [P1] asso-\nciates with [P2], [P1] binding to [P2], [P1],\ninhibitor of [P2].\n\nBetween-After: tokens between and after the two\ncandidate interacting entities. For instance:\n[P1] - [P2] association, [P1] and [P2] interact,\n[P1] has influence on [P2] binding.\n\nOur global context kernels operate on the patterns\nabove, where each pattern is represented using a\nbag-of-words instead of sparse subsequences of\nwords, PoS tags, entity and chunk types, or Word-\nNet synsets as in (Bunescu and Mooney, 2005b).\nMore formally, given a relation example R, we\nrepresent a pattern P as a row vector\n\n\ufffdP (R) = (tf(t1, P ), tf(t2, P ), . . . , tf(tl, P )) 2 Rl, (2)\n\nwhere the function tf(ti, P ) records how many\ntimes a particular token ti is used in P . Note that,\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\n\n\nthis approach differs from the standard bag-of-\nwords as punctuation and stop words are included\nin \ufffdP , while the entities (with attribute CANDI-\nDATE and OTHER) are not. To improve the clas-\nsification performance, we have further extended\n\ufffdP to embed n-grams of (contiguous) tokens (up\nto n = 3). By substituting \ufffdP into Equation 1, we\nobtain the n-gram kernel Kn, which counts com-\nmon uni-grams, bi-grams, . . . , n-grams that two\npatterns have in common2. The Global Context\nkernelKGC(R1, R2) is then defined as\n\nKFB(R1, R2) + KB(R1, R2) + KBA(R1, R2), (3)\n\nwhere KFB , KB and KBA are n-gram kernels\nthat operate on the Fore-Between, Between and\nBetween-After patterns respectively.\n\n3.2 Local Context Kernel\nThe type of the candidate interacting entities can\nprovide useful clues for detecting the agent and\ntarget of the relation, as well as the presence of the\nrelation itself. As the type is not known, we use\nthe information provided by the two local contexts\nof the candidate interacting entities, called left and\nright local context respectively. As typically done\nin entity recognition, we represent each local con-\ntext by using the following basic features:\n\nToken The token itself.\nLemma The lemma of the token.\nPoS The PoS tag of the token.\nOrthographic This feature maps each token into\n\nequivalence classes that encode attributes\nsuch as capitalization, punctuation, numerals\nand so on.\n\nFormally, given a relation example R, a local con-\ntext L = t\ufffdw, . . . , t\ufffd1, t0, t+1, . . . , t+w is repre-\nsented as a row vector\n\n L(R) = (f1(L), f2(L), . . . , fm(L)) 2 {0, 1}m, (4)\n\nwhere fi is a feature function that returns 1 if it is\nactive in the specified position of L, 0 otherwise3.\nThe Local Context kernelKLC(R1, R2) is defined\nas\n\nKleft(R1, R2) + Kright(R1, R2), (5)\n\nwhereKleft andKright are defined by substituting\nthe embedding of the left and right local context\ninto Equation 1 respectively.\n\n2In the literature, it is also called n-spectrum kernel.\n3In the reported experiments, we used a context window\n\nof \u00b12 tokens around the candidate entity.\n\nNotice that KLC differs substantially from\nKGC as it considers the ordering of the tokens and\nthe feature space is enriched with PoS, lemma and\northographic features.\n\n3.3 Shallow Linguistic Kernel\nFinally, the Shallow Linguistic kernel\nKSL(R1, R2) is defined as\n\nKGC(R1, R2) + KLC(R1, R2). (6)\n\nIt follows directly from the explicit construction\nof the feature space and from closure properties of\nkernels thatKSL is a valid kernel.\n\n4 Data sets\nThe two data sets used for the experiments concern\nthe same domain (i.e. gene/protein interactions).\nHowever, they present a crucial difference which\nmakes it worthwhile to show the experimental re-\nsults on both of them. In one case (AImed) in-\nteractions are considered symmetric, while in the\nother (LLL challenge) agents and targets of genic\ninteractions have to be identified.\n\n4.1 AImed corpus\nThe first data set used in the experiments is the\nAImed corpus4, previously used for training pro-\ntein interaction extraction systems in (Bunescu et\nal., 2005; Bunescu and Mooney, 2005b). It con-\nsists of 225 Medline abstracts: 200 are known\nto describe interactions between human proteins,\nwhile the other 25 do not refer to any interaction.\nThere are 4,084 protein references and around\n1,000 tagged interactions in this data set. In this\ndata set there is no distinction between genes and\nproteins and the relations are symmetric.\n\n4.2 LLL Challenge\nThis data set was used in the Learning Language\nin Logic (LLL) challenge on Genic Interaction\nextraction5 (Nede\u0301llec, 2005). The objective of\nthe challenge was to evaluate the performance of\nsystems based on machine learning techniques to\nidentify gene/protein interactions and their roles,\nagent or target. The data set was collected by\nquerying Medline on Bacillus subtilis transcrip-\ntion and sporulation. It is divided in a training set\n(80 sentences describing 271 interactions) and a\n\n4ftp://ftp.cs.utexas.edu/pub/mooney/\nbio-data/interactions.tar.gz\n\n5http://genome.jouy.inra.fr/texte/\nLLLchallenge/\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\n\n\ntest set (87 sentences describing 106 interactions).\nDifferently from the training set, the test set con-\ntains sentences without interactions. The data set\nis decomposed in two subsets of increasing diffi-\nculty. The first subset does not include corefer-\nences, while the second one includes simple cases\nof coreference, mainly appositions. Both subsets\nare available with different kinds of annotation:\nbasic and enriched. The former includes word and\nsentence segmentation. The latter also includes\nmanually checked information, such as lemma and\nsyntactic dependencies. A dictionary of named\nentities (including typographical variants and syn-\nonyms) is associated to the data set.\n\n5 Experiments\nBefore describing the results of the experiments,\na note concerning the evaluation methodology.\nThere are different ways of evaluating perfor-\nmance in extracting information, as noted in\n(Lavelli et al., 2004) for the extraction of slot\nfillers in the Seminar Announcement and the Job\nPosting data sets. Adapting the proposed classi-\nfication to relation extraction, the following two\ncases can be identified:\n\n\u2022 One Answer per Occurrence in the Document\n\u2013 OAOD (each individual occurrence of a\nprotein interaction has to be extracted from\nthe document);\n\n\u2022 One Answer per Relation in a given Docu-\nment \u2013 OARD (where two occurrences of the\nsame protein interaction are considered one\ncorrect answer).\n\nFigure 3 shows a fragment of tagged text drawn\nfrom the AImed corpus. It contains three different\ninteractions between pairs of proteins, for a total\nof seven occurrences of interactions. For example,\nthere are three occurrences of the interaction be-\ntween IGF-IR and p52Shc (i.e. number 1, 3 and\n7). If we adopt the OAOD methodology, all the\nseven occurrences have to be extracted to achieve\nthe maximum score. On the other hand, if we use\nthe OARD methodology, only one occurrence for\neach interaction has to be extracted to maximize\nthe score.\nOn the AImed data set both evaluations were\n\nperformed, while on the LLL challenge only the\nOAOD evaluation methodology was performed\nbecause this is the only one provided by the eval-\nuation server of the challenge.\n\nFigure 3: Fragment of the AImed corpus with all\nproteins and their interactions tagged. The pro-\ntein names have been highlighted in bold face and\ntheir same subscript numbers indicate interaction\nbetween the proteins.\n\n5.1 Implementation Details\nAll the experiments were performed using the\nSVM package LIBSVM6 customized to embed our\nown kernel. For the LLL challenge submission,\nwe optimized the regularization parameter C by\n10-fold cross validation; while we used its default\nvalue for the AImed experiment. In both exper-\niments, we set the cost-factor Wi to be the ratio\nbetween the number of negative and positive ex-\namples.\n\n5.2 Results on AImed\nKSL performance was first evaluated on the\nAImed data set (Section 4.1). We first give an\nevaluation of the kernel combination and then we\ncompare our results with the Subsequence Ker-\nnel for Relation Extraction (ERK) described in\n(Bunescu and Mooney, 2005b). All experiments\nare conducted using 10-fold cross validation on\nthe same data splitting used in (Bunescu et al.,\n2005; Bunescu and Mooney, 2005b).\nTable 1 shows the performance of the three ker-\n\nnels defined in Section 3 for protein-protein in-\nteractions using the two evaluation methodologies\ndescribed above.\nWe report in Figure 4 the precision-recall curves\n\nof ERK andKSL using OARD evaluation method-\nology (the evaluation performed by Bunescu and\nMooney (2005b)). As in (Bunescu et al., 2005;\nBunescu andMooney, 2005b), the graph points are\nobtained by varying the threshold on the classifi-\n\n6http://www.csie.ntu.edu.tw/\u02dccjlin/\nlibsvm/\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\nKiri L Wagstaff\n\n\n\n\nOAOD\nKernel Precision Recall F1\nKGC 57.7 60.1 58.9\nKLC 37.3 56.3 44.9\nKSL 60.9 57.2 59.0\n\nOARD\nKernel Precision Recall F1\nKGC 58.9 66.2 62.2\nKLC 44.8 67.8 54.0\nKSL 64.5 63.2 63.9\nERK 65.0 46.4 54.2\n\nTable 1: Performance on the AImed data set us-\ning the two evaluation methodologies, OAOD and\nOARD.\n\ncation confidence7. The results clearly show that\nKSL outperforms ERK, especially in term of re-\ncall (see Table 1).\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0 0.2 0.4 0.6 0.8 1\n\nP\nre\n\nc\nis\n\nio\nn\n\nRecall\n\nKSL vs. ERK\n\nERK\nKSL\n\nFigure 4: Precision-recall curves on the AImed\ndata set using OARD evaluation methodology.\n\nFinally, Figure 5 shows the learning curve of the\ncombined kernelKSL using the OARD evaluation\nmethodology. The curve reaches a plateau with\naround 100 Medline abstracts.\n\n5.3 Results on LLL challenge\nThe system was evaluated on the \u201cbasic\u201d version\nof the LLL challenge data set (Section 4.2).\nTable 2 shows the results of KSL returned by\n\nthe scoring service8 for the three subsets of the\ntraining set (with and without coreferences, and\nwith their union). Table 3 shows the best results\nobtained at the official competition performed in\nApril 2005. Comparing the results we see that\nKSL trained on each subset outperforms the best\n\n7For this purpose the probability estimate output of LIB-\nSVM is used.\n\n8http://genome.jouy.inra.fr/texte/\nLLLchallenge/scoringService.php\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0 50 100 150 200\n\nF\n1\n\nNumber of documents\n\nFigure 5: KSL learning curve on the AImed data\nset using OARD evaluation methodology.\n\nCoref. Precision Recall F1\nall 56.0 61.4 58.6\nwith 29.0 31.0 30.0\nwithout 54.8 62.9 58.6\n\nTable 2: KSL performance on the LLL challenge\ntest set using only the basic linguistic information.\n\nsystems of the LLL challenge9. Notice that the\nbest results at the challenge were obtained by dif-\nferent groups and exploiting the linguistic \u201cen-\nriched\u201d version of the data set. As observed in\n(Nede\u0301llec, 2005), the scores obtained using the\ntraining set without coreferences and the whole\ntraining set are similar.\nWe also report in Table 4 an analysis of the ker-\n\nnel combination. Given that we are interested here\nin the contribution of each kernel, we evaluated\nthe experiments by 10-fold cross-validation on the\nwhole training set avoiding the submission pro-\ncess.\n\n5.4 Discussion of Results\nThe experimental results show that the combined\nkernel KSL outperforms the basic kernels KGC\nandKLC on both data sets. In particular, precision\nsignificantly increases at the expense of a lower re-\ncall. High precision is particularly advantageous\nwhen extracting knowledge from large corpora,\nbecause it avoids overloading end users with too\nmany false positives.\nAlthough the basic kernels were designed to\n\nmodel complementary aspects of the task (i.e.\n9After the challenge deadline, Reidel and Klein (2005)\n\nachieved a significant improvement, F1 = 68.4% (without\ncoreferences) and F1 = 64.7% (with and without corefer-\nences).\n\n\n\nTest set Coref. Precision Recall F1\nEnriched all 55.6 53.0 54.3\n\nwith 29.0 31.0 24.4\nwithout 60.9 46.2 52.6\n\nBasic all n/a n/a n/a\nwith 14.0 82.7 24.0\nwithout 50.0 53.8 51.8\n\nTable 3: Best performance on basic and enriched\ntest sets obtained by participants in the official\ncompetition at the LLL challenge.\n\nKernel Precision Recall F1\nKGC 55.1 66.3 60.2\nKLC 44.8 60.1 53.8\nKSL 62.1 61.3 61.7\n\nTable 4: Comparison of the performance of kernel\ncombination on the LLL challenge using 10-fold\ncross validation.\n\npresence of the relation and roles of the interact-\ning entities), they perform reasonably well even\nwhen considered separately. In particular, KGC\nachieved good performance on both data sets. This\nresult was not expected on the LLL challenge be-\ncause this task requires not only to recognize the\npresence of relationships between entities but also\nto identify their roles. On the other hand, the out-\ncomes of KLC on the AImed data set show that\nsuch kernel helps to identify the presence of rela-\ntionships as well.\nAt first glance, it may seem strange that KGC\n\noutperforms ERK on AImed, as the latter ap-\nproach exploits a richer representation: sparse\nsub-sequences of words, PoS tags, entity and\nchunk types, or WordNet synsets. However, an\napproach based on n-grams is sufficient to identify\nthe presence of a relationship. This result sounds\nless surprising, if we recall that both approaches\ncast the relation extraction problem as a text cate-\ngorization task. Approaches to text categorization\nbased on rich linguistic information have obtained\nless accuracy than the traditional bag-of-words ap-\nproach (e.g. (Koster and Seutter, 2003)). Shallow\nlinguistics information seems to be more effective\nto model the local context of the entities.\nFinally, we obtained worse results performing\n\ndimensionality reduction either based on generic\nlinguistic assumptions (e.g. by removing words\nfrom stop lists or with certain PoS tags) or using\nstatistical methods (e.g. tf.idf weighting schema).\nThis may be explained by the fact that, in tasks like\nentity recognition and relation extraction, useful\n\nclues are also provided by high frequency tokens,\nsuch as stop words or punctuation marks, and by\nthe relative positions in which they appear.\n\n6 Related Work\n\nFirst of all, the obvious references for our work\nare the approaches evaluated on AImed and LLL\nchallenge data sets.\nIn (Bunescu and Mooney, 2005b), the authors\n\npresent a generalized subsequence kernel that\nworks with sparse sequences containing combina-\ntions of words and PoS tags.\nThe best results on the LLL challenge were ob-\n\ntained by the group from the University of Ed-\ninburgh (Reidel and Klein, 2005), which used\nMarkov Logic, a framework that combines log-\nlinear models and First Order Logic, to create a\nset of weighted clauses which can classify pairs of\ngene named entities as genic interactions. These\nclauses are based on chains of syntactic and se-\nmantic relations in the parse or Discourse Repre-\nsentation Structure (DRS) of a sentence, respec-\ntively.\nOther relevant approaches include those that\n\nadopt kernel methods to perform relation extrac-\ntion. Zelenko et al. (2003) describe a relation ex-\ntraction algorithm that uses a tree kernel defined\nover a shallow parse tree representation of sen-\ntences. The approach is vulnerable to unrecover-\nable parsing errors. Culotta and Sorensen (2004)\ndescribe a slightly generalized version of this ker-\nnel based on dependency trees, in which a bag-of-\nwords kernel is used to compensate for errors in\nsyntactic analysis. A further extension is proposed\nby Zhao and Grishman (2005). They use compos-\nite kernels to integrate information from different\nsyntactic sources (tokenization, sentence parsing,\nand deep dependency analysis) so that process-\ning errors occurring at one level may be overcome\nby information from other levels. Bunescu and\nMooney (2005a) present an alternative approach\nwhich uses information concentrated in the short-\nest path in the dependency tree between the two\nentities.\nAs mentioned in Section 1, another relevant ap-\n\nproach is presented in (Roth and Yih, 2002). Clas-\nsifiers that identify entities and relations among\nthem are first learned from local information in\nthe sentence. This information, along with con-\nstraints induced among entity types and relations,\nis used to perform global probabilistic inference\n\n\n\nthat accounts for the mutual dependencies among\nthe entities.\nAll the previous approaches have been evalu-\n\nated on different data sets so that it is not possi-\nble to have a clear idea of which approach is better\nthan the other.\n\n7 Conclusions and Future Work\n\nThe good results obtained using only shallow lin-\nguistic features provide a higher baseline against\nwhich it is possible to measure improvements ob-\ntained using methods based on deep linguistic pro-\ncessing. In the near future, we plan to extend our\nwork in several ways.\nFirst, we would like to evaluate the contribu-\n\ntion of syntactic information to relation extraction\nfrom biomedical literature. With this aim, we will\nintegrate the output of a parser (possibly trained on\na domain-specific resource such the Genia Tree-\nbank). Second, we plan to test the portability of\nour model on ACE and MUC data sets. Third,\nwe would like to use a named entity recognizer\ninstead of assuming that entities are already ex-\ntracted or given by a dictionary. Our long term\ngoal is to populate databases and ontologies by\nextracting information from large text collections\nsuch as Medline.\n\n8 Acknowledgements\n\nWe would like to thank Razvan Bunescu for pro-\nviding detailed information about the AImed data\nset and the settings of the experiments. Clau-\ndio Giuliano and Lorenza Romano have been sup-\nported by the ONTOTEXT project, funded by the\nAutonomous Province of Trento under the FUP-\n2004 research program.\n\nReferences\nRazvan Bunescu and Raymond J. Mooney. 2005a.\nA shortest path dependency kernel for relation ex-\ntraction. In Proceedings of the Human Language\nTechnology Conference and Conference on Empiri-\ncal Methods in Natural Language Processing, Van-\ncouver, B.C, October.\n\nRazvan Bunescu and Raymond J. Mooney. 2005b.\nSubsequence kernels for relation extraction. In\nProceedings of the 19th Conference on Neural In-\nformation Processing Systems, Vancouver, British\nColumbia.\n\nRazvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-\nward M. Marcotte, Raymond J. Mooney, Arun K.\n\nRamani, and Yuk Wah Wong. 2005. Comparative\nexperiments on learning information extractors for\nproteins and their interactions. Artificial Intelligence\nin Medicine, 33(2):139\u2013155. Special Issue on Sum-\nmarization and Information Extraction from Medi-\ncal Documents.\n\nAron Culotta and Jeffrey Sorensen. 2004. Dependency\ntree kernels for relation extraction. In Proceedings\nof the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL 2004), Barcelona,\nSpain.\n\nAlfio Gliozzo, Claudio Giuliano, and Carlo Strappar-\nava. 2005. Domain kernels for word sense disam-\nbiguation. In Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2005), Ann Arbor, Michigan, June.\n\nCornelis H. A. Koster and Mark Seutter. 2003. Taming\nwild phrases. In Advances in Information Retrieval,\n25th European Conference on IR Research (ECIR\n2003), pages 161\u2013176, Pisa, Italy.\n\nAlberto Lavelli, Mary Elaine Califf, Fabio Ciravegna,\nDayne Freitag, Claudio Giuliano, Nicholas Kushm-\nerick, and Lorenza Romano. 2004. IE evaluation:\nCriticisms and recommendations. In Proceedings of\nthe AAAI 2004 Workshop on Adaptive Text Extrac-\ntion and Mining (ATEM 2004), San Jose, California.\n\nClaire Nede\u0301llec. 2005. Learning language in logic -\ngenic interaction extraction challenge. In Proceed-\nings of the ICML-2005 Workshop on Learning Lan-\nguage in Logic (LLL05), pages 31\u201337, Bonn, Ger-\nmany, August.\n\nSebastian Reidel and Ewan Klein. 2005. Genic\ninteraction extraction with semantic and syntactic\nchains. In Proceedings of the ICML-2005 Workshop\non Learning Language in Logic (LLL05), pages 69\u2013\n74, Bonn, Germany, August.\n\nD. Roth and W. Yih. 2002. Probabilistic reasoning\nfor entity & relation recognition. In Proceedings of\nthe 19th International Conference on Computational\nLinguistics (COLING-02), Taipei, Taiwan.\n\nJohn Shawe-Taylor and Nello Cristianini. 2004. Ker-\nnel Methods for Pattern Analysis. Cambridge Uni-\nversity Press, New York, NY, USA.\n\nVladimir Vapnik. 1998. Statistical Learning Theory.\nJohn Wiley and Sons, New York.\n\nDmitry Zelenko, Chinatsu Aone, and Anthony\nRichardella. 2003. Kernel methods for information\nextraction. Journal of Machine Learning Research,\n3:1083\u20131106.\n\nShubin Zhao and Ralph Grishman. 2005. Extracting\nrelations with integrated information using kernel\nmethods. In Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2005), Ann Arbor, Michigan, June.", "file": "/Users/ksingh/mte/papers/giuliano-shallow-rel.pdf", "metadata": {"access_permission:can_modify": "true", "access_permission:extract_content": "true", "producer": "Mac OS X 10.11.6 Quartz PDFContext", "grobid:header_TEIXMLSource": "[PARSING_ERROR] Cannot parse file: /Users/ksingh/git-workspace/grobid/grobid-home/tmp/vehvGySwAj.lxml", "NER_PERCENT": ["68.4%", "64.7%"], "meta:save-date": "2017-02-06T20:16:12Z", "Last-Modified": "2017-02-06T20:16:12Z", "NER_PERSON": ["Ewan Klein", "Lorenza Romano ITC-irst Via Sommarive", "Nello Cristianini", "Cornelis H. A. Koster", "Nicholas Kushm", "M. Marcotte", "Yuk Wah Wong", "Mark Seutter", "rea", "Shubin Zhao", "Aron Culotta", "Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff", "Koster", "Alberto Lavelli", "Mooney", "Culotta", "Zelenko", "Arun K. Ramani", "Claudio Giuliano", "Sebastian Reidel", "Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff", "Ralph Grishman", "Roth", "Dayne Freitag", "Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff", "W. Yih", "Ann Arbor", "Dmitry Zelenko", "Fabio Ciravegna", "Grishman", "Vladimir Vapnik", "Subsequence Ker", "Lorenza Romano", "Alfio Gliozzo", "Zhao", "Giuliano", "John Wiley", "Mary Elaine Califf", "Raymond J. Mooney", "Anthony Richardella", "Lavelli", "John Shawe-Taylor", "Razvan Bunescu", "Rohit J. Kate", "Klein", "Jeffrey Sorensen", "Chinatsu Aone", "Carlo Strappar", "Sorensen"], "xmp:CreatorTool": "TeX", "dcterms:created": "2017-02-06T20:16:12Z", "access_permission:can_print": "true", "resourceName": "giuliano-shallow-rel.pdf", "X-Parsed-By": ["org.apache.tika.parser.CompositeParser", "org.apache.tika.parser.journal.JournalParser", "org.apache.tika.parser.ner.NamedEntityParser"], "pdf:docinfo:created": "2017-02-06T20:16:12Z", "access_permission:extract_for_accessibility": "true", "access_permission:fill_in_form": "true", "pdf:encrypted": "false", "pdf:docinfo:producer": "Mac OS X 10.11.6 Quartz PDFContext", "dc:format": "application/pdf; version=1.3", "NER_ORGANIZATION": ["Kn", "KLC", "Human Language Technology Conference and Conference", "Autonomous Province of Trento", "KFB", "the Association for Computational Linguistics", "LLL", "nel Methods for Pattern Analysis", "sec", "cal Documents", "Machine Learning Research", "19th International Conference on Computational Linguistics", "Cambridge Uni", "Vector Machines", "Markov Logic", "Experiments Before", "/genome.jouy.inra.fr/texte/ LLLchallenge/scoringService", "FUP", "USA", "Ruifang Ge", "KSL", "KGC", "University of Ed", "Vapnik", "Natural Language Processing", "Equation 1", "IR Research", "Kernel Precision Recall F1 KGC", "British Columbia", "42nd Annual Meeting of the Association for Computational Linguistics", "Medline", "Kiri L Wagstaff Kiri L Wagstaff Kiri L Wagstaff OAOD Kernel Precision Recall F1 KGC", "19th Conference", "First Order Logic", "Exploiting Shallow Linguistic Information for Relation Extraction", "Shallow Linguistic Kernel Finally", "Bunescu", "Information Retrieval", "Kernel Methods for Relation Extraction"], "grobid:header_TEIJSONSource": "{}", "NER_LOCATION": ["New York", "Vancouver", "Pisa", "Barcelona", "Taipei", "Empiri", "California", "San Jose", "Spain", "Michigan", "Genia", "Bonn", "Taiwan", "Italy", "Bunescu", "Germany"], "Creation-Date": "2017-02-06T20:16:12Z", "pdf:docinfo:creator_tool": "TeX", "pdf:docinfo:modified": "2017-02-06T20:16:12Z", "grobid:header_Class": "org.apache.tika.metadata.Metadata", "access_permission:assemble_document": "true", "NER_DATE": ["1998", "June", "1106", "October", "2005", "2004", "2003", "Figure 1", "April 2005", "August"], "Last-Save-Date": "2017-02-06T20:16:12Z", "X-TIKA:parse_time_millis": "222", "date": "2017-02-06T20:16:12Z", "created": "Mon Feb 06 12:16:12 PST 2017", "modified": "2017-02-06T20:16:12Z", "meta:creation-date": "2017-02-06T20:16:12Z", "access_permission:modify_annotations": "true", "pdf:PDFVersion": "1.3", "xmpTPg:NPages": "8", "access_permission:can_print_degraded": "true", "dcterms:modified": "2017-02-06T20:16:12Z", "Content-Type": "application/pdf"}}
